<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Bayesian Nonparametric Clustering Algorithm · ThunderBayes.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.044/juliamono.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ThunderBayes.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ThunderBayes.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">ThunderBayes.jl</a></li><li class="is-active"><a class="tocitem" href>Bayesian Nonparametric Clustering Algorithm</a><ul class="internal"><li><a class="tocitem" href="#Abstract"><span>Abstract</span></a></li><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Background"><span>Background</span></a></li><li><a class="tocitem" href="#API-Methods"><span>API Methods</span></a></li><li><a class="tocitem" href="#Simulation"><span>Simulation</span></a></li><li><a class="tocitem" href="#Conclusions"><span>Conclusions</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Bayesian Nonparametric Clustering Algorithm</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Bayesian Nonparametric Clustering Algorithm</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/jirotubuyaki/ThunderBayes/blob/main/docs/src/Algorithms/crp_normal.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Bayesian-Nonparametric-Clustering-Algorithm"><a class="docs-heading-anchor" href="#Bayesian-Nonparametric-Clustering-Algorithm">Bayesian Nonparametric Clustering Algorithm</a><a id="Bayesian-Nonparametric-Clustering-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Nonparametric-Clustering-Algorithm" title="Permalink"></a></h1><h2 id="Abstract"><a class="docs-heading-anchor" href="#Abstract">Abstract</a><a id="Abstract-1"></a><a class="docs-heading-anchor-permalink" href="#Abstract" title="Permalink"></a></h2><p>Clustering is a scientific method which finds the clusters of data and many related methods are traditionally researched. Bayesian nonparametrics is statistics which can treat models having infinite parameters. Chinese restaurant process is used in order to compose Dirichlet process. The clustering which uses Chinese restaurant process does not need to decide the number of clusters in advance. This algorithm automatically adjusts it. Then, this package can calculate clusters in addition to entropy as the ambiguity of clusters.</p><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Clustering is an analytical method in order to find the clusters of data and many related methods are proposed. K-means[1] and Hierarchical clustering[2] are famous algorithmic methods. Density-based clustering[3] is the method that finds clusters by calculating a concentration of data. In statistical methods, there are stochastic ways such as bayesian clustering[4]. However these methods need to decide the number of clusters in advance. Therefore if the data is both high dimensions and a complex, deciding the accurate number of clusters is difficult. Bayesian nonparametric method[5] composes infinite parameters by Dirichlet process[6]. Dirichlet process is the infinite dimensional discrete distribution that is composed by Stocastic processes like a chinese restaurant process (CRP)[7] or stick-breaking process[8]. CRP does not need to decide the number of clusters in advance. This algorithm automatically adjusts it. We implement the CRP Clustering and the method which calculates the entropy[9] into R package. Then, we explain the clustering model and how to use it in detail and execute simulation by example datasets.</p><h2 id="Background"><a class="docs-heading-anchor" href="#Background">Background</a><a id="Background-1"></a><a class="docs-heading-anchor-permalink" href="#Background" title="Permalink"></a></h2><h3 id="Chinese-Restaurant-Process"><a class="docs-heading-anchor" href="#Chinese-Restaurant-Process">Chinese Restaurant Process</a><a id="Chinese-Restaurant-Process-1"></a><a class="docs-heading-anchor-permalink" href="#Chinese-Restaurant-Process" title="Permalink"></a></h3><p>Chinese restaurant process is a metaphor looks like customers sit at a table in Chinese restaurant. All customers except for x<em>i have already sat at finite tables. A new customer x</em>i will sit at either a table which other customers have already sat at or a new table. A new customer tends to sit at a table which has the number of customers more than other tables. A probability equation is given by    </p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/equation_1.png?raw=true" alt="equa"/></p><p>where n^i_k denotes the number of the customers at a table k except for i and α is a concentration parameter.</p><h3 id="Markov-Chain-Monte-Carlo-Methods-for-Clustering"><a class="docs-heading-anchor" href="#Markov-Chain-Monte-Carlo-Methods-for-Clustering">Markov Chain Monte Carlo Methods for Clustering</a><a id="Markov-Chain-Monte-Carlo-Methods-for-Clustering-1"></a><a class="docs-heading-anchor-permalink" href="#Markov-Chain-Monte-Carlo-Methods-for-Clustering" title="Permalink"></a></h3><p>Markov chain Monte Carlo (MCMC) methods[10] are algorithmic methods to sample from posterior distributions. If conditional posterior distributions are given by models, it is the best way in order to acquire parameters from posterior distributions. The algorithm for this package is given by  </p><p>i) ii) iterations continue on below:  </p><p>i) Sampling z_i for each i (i = 1,2, ・・・,n)</p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/equation_2.png?raw=true" alt="equa"/></p><p>where k is a k th cluster and i is a i th data. μ new and Σ new are calculated from all data.</p><p>ii) Calculating parameters for each k (k = 1,2, ・・・,∞)</p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/equation_3.png?raw=true &quot;eque&quot;" alt="equa"/></p><p>Iterations i) ii) continue by iteration number, and Σ k is a variance-covariance matrix of kth cluster. Cov is covariance. i and j are rows and columns’ number of Σ k ij and Σ new ij . First several durations of iterations which are called as burn<em>in are error ranges. For that reason, burn</em>in durations are abandoned.</p><h3 id="Clusters-Entropy"><a class="docs-heading-anchor" href="#Clusters-Entropy">Clusters Entropy</a><a id="Clusters-Entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Clusters-Entropy" title="Permalink"></a></h3><p>Entropy denotes the ambiguity of clustering. As a result of a simulation, data x i joins in a particular cluster. From the total numbers n k of the particular cluster k at the last iteration, a probability p k at each cluster k is calculated. The entropy equation is given by</p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/equation_4.png?raw=true &quot;eque&quot;" alt="equa"/></p><h2 id="API-Methods"><a class="docs-heading-anchor" href="#API-Methods">API Methods</a><a id="API-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#API-Methods" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="ThunderBayes.ThunderBayes" href="#ThunderBayes.ThunderBayes"><code>ThunderBayes.ThunderBayes</code></a> — <span class="docstring-category">Module</span></header><section><div><pre><code class="language-julia hljs">ThunderBayes</code></pre><p>Bayesian Nonparametric Clustering Algorithms</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jirotubuyaki/ThunderBayes/blob/7d91504421106eabac32358147aadd50d9038ddc/src/ThunderBayes.jl#L6-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ThunderBayes.data_check" href="#ThunderBayes.data_check"><code>ThunderBayes.data_check</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">data_check(data)</code></pre><p>Check whether data contain NaN value.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">    ok = data_check(data)</code></pre><p><strong>Arguments</strong></p><ul><li><code>data::Array</code> : the colums are the values of dimentions.</li></ul><p><strong>Return</strong></p><ul><li><code>ok::Bool</code> : whether data contain NaN or not.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jirotubuyaki/ThunderBayes/blob/7d91504421106eabac32358147aadd50d9038ddc/src/ThunderBayes.jl#L20-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ThunderBayes.crp_train" href="#ThunderBayes.crp_train"><code>ThunderBayes.crp_train</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">crp_train</code></pre><p>Calculate the clusters of data. MCMC Algorithms have been implemented. </p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">burn_in = 5000
iteration = 50000
mu = [0, 0, 0, 0, 0]
sigma_table = Matrix{Float64}(1 * I, 5, 5)
alpha = 1
ro_0 = 1
autoparams = false
result, cluster_id = crp_train(data, burn_in, iteration, mu, sigma_table, alpha, ro_0, auto_params)</code></pre><p><strong>Arguments</strong></p><ul><li><code>data::Array</code> : the colums are the values of dimentions.</li><li><code>burn_in::Integer</code> : an iteration integer of burn in.  burn in duration is abandoned.</li><li><code>iteration::Integer</code> : an iteration integer.   </li><li><code>mu::Array</code> : a vector of center points of data. If data is 3 dimensions, a vector of 3 elements like &quot;[2, 4, 1]&quot;.  </li><li><code>sigma_table::Matrx</code> : a numeric of table position variance.  </li><li><code>alpha::Integer=1</code> : a numeric of a CRP concentration rate.  </li><li><code>ro_0::Integer=1</code> : a numeric of a CRP mu change rate.</li><li><code>autoparams::Bool</code> : automate sigma_table and mu for your data.</li></ul><p><strong>Return</strong></p><ul><li><code>Result::Array</code> : the mean values and variance-covariance matrix of the Clusters .</li><li><code>cluster_id::Vector</code> : the cluster id of data. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jirotubuyaki/ThunderBayes/blob/7d91504421106eabac32358147aadd50d9038ddc/src/ThunderBayes.jl#L53-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ThunderBayes.crp_predict" href="#ThunderBayes.crp_predict"><code>ThunderBayes.crp_predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">crp_predict(data, result)</code></pre><p>Prediction for new data.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">    prediction_result = crp_preduct(data, cluster_id)</code></pre><p><strong>Arguments</strong></p><ul><li><code>data::vector</code> : a data for prediction.</li><li><code>result::Array</code> : a return variable of function crp_train() .</li></ul><p><strong>Return</strong></p><ul><li><code>prediction_result::Array</code> : the first column is cluster id and next colums are joined probabiilty of each clusters.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jirotubuyaki/ThunderBayes/blob/7d91504421106eabac32358147aadd50d9038ddc/src/ThunderBayes.jl#L509-L528">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="ThunderBayes.crp_visualize" href="#ThunderBayes.crp_visualize"><code>ThunderBayes.crp_visualize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">crp_visualize(data, cluster_id)</code></pre><p>Visualization of clustering results.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">    crp_visualize(data, cluster_id)</code></pre><p><strong>Arguments</strong></p><ul><li><code>data::Array</code> : a colums are the values of dimentions.</li><li><code>cluster_id::Vector</code> : a return variable of function crp_train() .</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/jirotubuyaki/ThunderBayes/blob/7d91504421106eabac32358147aadd50d9038ddc/src/ThunderBayes.jl#L489-L503">source</a></section></article><h2 id="Simulation"><a class="docs-heading-anchor" href="#Simulation">Simulation</a><a id="Simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Simulation" title="Permalink"></a></h2><p>We use dataset from Clustering basic benchmark( http://cs.joensuu.fi/sipu/datasets/ )[11]. If increase α parameter, new clusters tend to increase. burin_in iterations are abandoned. The result is plotted and each data joins in any cluster. The graph is given by below:</p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/Rplot01.png?raw=true &quot;eque&quot;" alt="equa"/></p><p>Figure 1: Aggregation: Data is 788 elements and 2 dimentions. Parameters are set as alpha=1, burnin=100, iteration=1000.  </p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/Rplot02.png?raw=true &quot;eque&quot;" alt="equa"/>  </p><p>Figure 2: 3 normal distribution: Data is 1000 elements and 2 dimentions. Parameters are set as alpha=0.5, burnin=100, iteration=1000.  </p><p><img src="https://github.com/jirotubuyaki/jirotubuyaki.github.io/blob/master/readme_images/Rplot.png?raw=true &quot;eque&quot;" alt="equa"/>  </p><p>Figure 3: 10 dimentional normal distributions: Data is generated from 10 dimentional normal distributions and parameters are set as alpha=1, burnin=100, iteration=1000.  </p><h2 id="Conclusions"><a class="docs-heading-anchor" href="#Conclusions">Conclusions</a><a id="Conclusions-1"></a><a class="docs-heading-anchor-permalink" href="#Conclusions" title="Permalink"></a></h2><p>Chinese restaurant process clustering was implemented and explained how to use it. Computer resources are limited. Computer processing power is the most important problem. After this, several improvements are planed. Please send suggestions and report bugs to okadaalgorithm@gmail.com.</p><h3 id="Acknowledgments"><a class="docs-heading-anchor" href="#Acknowledgments">Acknowledgments</a><a id="Acknowledgments-1"></a><a class="docs-heading-anchor-permalink" href="#Acknowledgments" title="Permalink"></a></h3><p>This activity would not have been possible without the support of my family and friends. To my family, thank you for much encouragement for me and inspiring me to follow my dreams. I am especially grateful to my parents, who supported me all aspects.  </p><h3 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h3><p>[1] Hartigan, J. A.; Wong, M. A. Algorithmas136: A k-means clustering algorithm . Journal of the Royal Statistical Society, Series C. 28 (1): 100–108. JSTOR 2346830, 1979.  </p><p>[2] Rokach, Lior, and Oded Maimon. &quot;Clustering methods.&quot; Data mining and knowledge discovery handbook. Springer US, 2005. 321-352.  </p><p>[3] Ester, Martin; Kriegel, Hans-Peter; Sander, Jörg; Xu, Xiaowei (1996). Simoudis, Evangelos; Han, Jiawei; Fayyad, Usama M. (eds.). A density-based algorithm for discovering clusters in large spatial databases with noise. Proceedings of the Second International Con ference on Knowledge Discovery and Data Mining (KDD-96). AAAI Press. pp. 226–231.  </p><p>[4] John W Lau &amp; Peter J Green (2007) Bayesian Model-Based Clustering Procedures, Journal of Computational and Graphical Statistics, 16:3, 526-558, DOI: 10.1198/106186007X238855.  </p><p>[5] Muller Peter, et al. Bayesian Nonparametric Data Analysis. Springer, 2015.  </p><p>[6] Ferguson, Thomas. Bayesian analysis of some nonparametric problems. Annals of Statistics. 1 (2): 209–230., 1973.  </p><p>[7] Pitman, Jim. Exchangeable and partially exchangeable random partitions. Probability Theory and Related Fields 102 (2): 145–158., 1995.  </p><p>[8] Broderick, Tamara, et al. “Beta Processes, Stick-Breaking and Power Laws.” Bayesian Analysis, vol. 7, no. 2, 2012, pp. 439–476., doi:10.1214/12-ba715.  </p><p>[9] Elliott H. Lieb; Jakob Yngvason. The physics and mathematics of the second law of thermodynamics. Physics Reports Volume:310 Issue:1 1-96., 1999.  </p><p>[10] Liu, Jun S. The collapsed gibbs sampler in bayesian computations with applications to a gene regulation problem. Journal of the American Statistical Association 89 (427): 958–966., 1994.  </p><p>[11] P. Fränti and S. Sieranoja K-means properties on six clustering benchmark datasets. Applied Intelligence, 48 (12), 4743-4759, December 2018.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« ThunderBayes.jl</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.19 on <span class="colophon-date" title="Friday 1 July 2022 06:34">Friday 1 July 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
